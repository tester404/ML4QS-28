{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelle/anaconda3/envs/ML4QS/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 2                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.dates as md\n",
    "\n",
    "\n",
    "class CreateDataset():\n",
    "\n",
    "    base_dir = ''\n",
    "    granulairity = 0\n",
    "    data_table = None\n",
    "\n",
    "    def __init__(self, base_dir, granularity):\n",
    "        self.base_dir = base_dir\n",
    "        self.granularity = granularity\n",
    "\n",
    "    # Create an initial data table with entries from start till end time, with steps\n",
    "    # of size granularity. Granularity is specified in milliseconds\n",
    "    def create_timestamps(self, start_time, end_time):\n",
    "        return pd.date_range(start_time, end_time, freq=str(self.granularity)+'ms')\n",
    "\n",
    "    def create_dataset(self, start_time, end_time, cols, prefix):\n",
    "        c = copy.deepcopy(cols)\n",
    "        if not prefix == '':\n",
    "            for i in range(0, len(c)):\n",
    "                c[i] = str(prefix) + str(c[i])\n",
    "        timestamps = self.create_timestamps(start_time, end_time)\n",
    "        self.data_table = pd.DataFrame(index=timestamps, columns=c)\n",
    "\n",
    "    # Add numerical data, we assume timestamps in the form of nanoseconds from the epoch\n",
    "    def add_numerical_dataset(self, file, timestamp_col, value_cols, aggregation='avg', prefix=''):\n",
    "        dataset = pd.read_csv(self.base_dir + file, skipinitialspace=True)\n",
    "\n",
    "        # Convert timestamps to dates\n",
    "        dataset[timestamp_col] = pd.to_datetime(dataset[timestamp_col])\n",
    "\n",
    "        # Create a table based on the times found in the dataset\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[timestamp_col]), max(dataset[timestamp_col]), value_cols, prefix)\n",
    "        else:\n",
    "            for col in value_cols:\n",
    "                self.data_table[str(prefix) + str(col)] = np.nan\n",
    "\n",
    "        # Over all rows in the new table\n",
    "        for i in range(0, len(self.data_table.index)):\n",
    "            # Select the relevant measurements.\n",
    "            relevant_rows = dataset[\n",
    "                (dataset[timestamp_col] >= self.data_table.index[i]) &\n",
    "                (dataset[timestamp_col] < (self.data_table.index[i] +\n",
    "                                           timedelta(milliseconds=self.granularity)))\n",
    "            ]\n",
    "            for col in value_cols:\n",
    "                # Take the average value\n",
    "                if len(relevant_rows) > 0:\n",
    "                    if aggregation == 'avg':\n",
    "                        self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.average(relevant_rows[col])\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n",
    "                else:\n",
    "                    self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.nan\n",
    "\n",
    "    # Remove undesired value from the names.\n",
    "    def clean_name(self, name):\n",
    "        return re.sub('[^0-9a-zA-Z]+', '', name)\n",
    "\n",
    "    # Add data in which we have rows that indicate the occurrence of a certain event with a given start and end time.\n",
    "    # 'aggregation' can be 'sum' or 'binary'.\n",
    "    def add_event_dataset(self, file, start_timestamp_col, end_timestamp_col, value_col, aggregation='sum'):\n",
    "        dataset = pd.read_csv(self.base_dir + file)\n",
    "\n",
    "        # Convert timestamps to datetime.\n",
    "        dataset[start_timestamp_col] = pd.to_datetime(dataset[start_timestamp_col])\n",
    "        dataset[end_timestamp_col] = pd.to_datetime(dataset[end_timestamp_col])\n",
    "\n",
    "        # Clean the event values in the dataset\n",
    "        dataset[value_col] = dataset[value_col].apply(self.clean_name)\n",
    "        event_values = dataset[value_col].unique()\n",
    "\n",
    "        # Add columns for all possible values (or create a new dataset if empty), set the default to 0 occurrences\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[start_timestamp_col]), max(dataset[end_timestamp_col]), event_values, value_col)\n",
    "        for col in event_values:\n",
    "            self.data_table[(str(value_col) + str(col))] = 0\n",
    "\n",
    "        # Now we need to start counting by passing along the rows....\n",
    "        for i in range(0, len(dataset.index)):\n",
    "            # identify the time points of the row in our dataset and the value\n",
    "            start = dataset[start_timestamp_col][i]\n",
    "            end = dataset[end_timestamp_col][i]\n",
    "            value = dataset[value_col][i]\n",
    "            border = (start - timedelta(milliseconds=self.granularity))\n",
    "\n",
    "            # get the right rows from our data table\n",
    "            relevant_rows = self.data_table[(start <= (self.data_table.index +timedelta(milliseconds=self.granularity))) & (end > self.data_table.index)]\n",
    "\n",
    "            # and add 1 to the rows if we take the sum\n",
    "            if aggregation == 'sum':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] += 1\n",
    "            # or set to 1 if we just want to know it happened\n",
    "            elif aggregation == 'binary':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] = 1\n",
    "            else:\n",
    "                raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n",
    "\n",
    "    # This function returns the column names that have one of the strings expressed by 'ids' in the column name.\n",
    "    def get_relevant_columns(self, ids):\n",
    "        relevant_dataset_cols = []\n",
    "        cols = list(self.data_table.columns)\n",
    "\n",
    "        for id in ids:\n",
    "            relevant_dataset_cols.extend([col for col in cols if id in col])\n",
    "\n",
    "        return relevant_dataset_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
